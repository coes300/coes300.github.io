<!DOCTYPE html>
<html>
<head>
    <title>Testing Machine Learning Models for Australian Football League Game Predictions</title>
</head>
<body>
    <h1>Testing Machine Learning Models for Australian Football League Game Predictions</h1>
    <h2>Project Overview</h2>
    <p>
        Originally made for the use in a weekly tipping league, the objective was to leverage machine learning techniques in predicting future AFL games using machine learning models.
    </p>
    <p>
        The primary dataset, downloaded from <a href="https://www.kaggle.com/datasets/gabrieldennis/afl-player-and-game-data-and-statistics18972022">kaggle/a>
        consists of AFL game-by-game results from 1965 to 2022. The data preprocessing, model creation, and subsequent training and testing processes are broken down into four python scripts: data.py, ridgeregression.py, xg_basic.py, and xg_adv.py.
    </p>
    <h2>Data Preprocessing - data.py</h2>
    <p>
        The initial step in our data processing pipeline involves data cleaning, transformation, and feature engineering. To accomplish this, we rely on the python script data.py.
    </p>
    <p>
        The raw dataset is loaded from a CSV file 'afl_game_by_game_results_1965_2022.csv'. Several columns, mostly focused on specific game events like 'tackles', 'rebound_50s', 'inside_50s', 'clearances', etc., are removed to reduce complexity and dimensionality. The dataset is then sorted by 'year' and 'round' to maintain chronological order.
    </p>
    <p>
        Subsequently, specific columns like 'goals' and 'behinds' are split into separate features for each team. Furthermore, additional fields such as 'team_score' and 'opponent_score' are derived from these. A binary target feature 'won' is created, signifying whether the team won the match or not.
    </p>
    <p>
        Columns containing numeric game data are split into team and opponent specific statistics. The dataframe is then grouped by the 'team' column, and the 'target' feature is shifted to align with future games, representing our prediction target. Finally, any rows containing missing values in key columns are removed.
    </p>
    <h2>Ridge Regression - ridgeregression.py</h2>
    <p>
    The ridgeregression.py script adopts the Ridge Regression model, a regularized linear regression variant that combats overfitting and handles multicollinearity. 
        The model uses a time-series cross-validation method and Sequential Feature Selector for optimal performance assessment and feature selection. 
        A rolling-window strategy is used to account for potential temporal dependencies between games, training, and evaluating the model incrementally as new game data becomes available. 
        The predictive accuracy of this approach is observed to be 0.605.
    </p>
    <h2>Basic XGBoost Model - xg_basic.py</h2>
    <p>
        The xg_basic.py script introduces a Gradient Boosting model using XGBoost (eXtreme Gradient Boosting), a machine learning algorithm based on the gradient boosting framework.
    </p>
    <p>
         XGBoost classifier. The XGBoost algorithm is an ensemble learning method, which often provides high predictive accuracy.

        In this script, certain columns are dropped from the data, and then the remaining columns are scaled using a MinMaxScaler. 
        The model is trained on data from years before 2019 and tested on data from years 2019 and onwards. After this, predictions are made and the accuracy is calculated, obtaining a value of 0.572.
    </p>
    <h2>Advanced XGBoost Model - xg_adv.py</h2>
    <p>
        This script is an advanced version of the previous XGBoost script. It includes a grid search cross-validation strategy to identify the best hyperparameters for the XGBoost model. 
        The parameter grid includes different values for max_depth, learning_rate, n_estimators, min_child_weight, gamma, subsample, and colsample_bytree.
    </p>
    <p>
        Once the optimal hyperparameters are identified, the model is retrained with these settings and used to predict the outcomes of games from 2019 onwards. The model's performance is evaluated based on its accuracy. 
        Walk-forward validation was utilized to simulate a real-world scenario where the model is updated as new data becomes available. For each game in the test set, the model was retrained with all the data available up to that game and then made a prediction for the game's outcome. 
    <p>
        All three scripts aim to predict a target variable based on various features, but they use different strategies and models. 

The accuracy of the models varies, with the Ridge Classifier obtaining an accuracy of 0.605, the basic XGBoost model obtaining an accuracy of 0.572, and the advanced XGBoost model obtaining an accuracy of 
        0.680 (before walk-forward validation, after which the accuracy drops to 0.487). This indicates that the advanced XGBoost model (before walk-forward validation) are the most accurate.
    </p>
</body>
</html>
