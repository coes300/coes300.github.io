<!DOCTYPE html>
<html>
<head>
    <title>Testing Machine Learning Models for Australian Football League Game Predictions</title>
</head>
<body>
    <h1>Testing Machine Learning Models for Australian Football League Game Predictions</h1>
    <h2>Project Overview</h2>
    <p>
        The scope of this project revolves around the prediction of future Australian Football League (AFL) games using sophisticated machine learning models. The objective is to forecast game outcomes accurately and improve predictive performance by leveraging both basic and advanced machine learning techniques.
    </p>
    <p>
        The primary dataset consists of AFL game-by-game results from 1965 to 2022. The data preprocessing, model creation, and subsequent training and testing processes are broken down into four python scripts: data.py, ridgeregression.py, xg_basic.py, and xg_adv.py.
    </p>
    <h2>Data Preprocessing - data.py</h2>
    <p>
        The initial step in our data processing pipeline involves data cleaning, transformation, and feature engineering. To accomplish this, we rely on the python script data.py.
    </p>
    <p>
        The raw dataset is loaded from a CSV file 'afl_game_by_game_results_1965_2022.csv'. Several columns, mostly focused on specific game events like 'tackles', 'rebound_50s', 'inside_50s', 'clearances', etc., are removed to reduce complexity and dimensionality. The dataset is then sorted by 'year' and 'round' to maintain chronological order.
    </p>
    <p>
        Subsequently, specific columns like 'goals' and 'behinds' are split into separate features for each team. Furthermore, additional fields such as 'team_score' and 'opponent_score' are derived from these. A binary target feature 'won' is created, signifying whether the team won the match or not.
    </p>
    <p>
        Columns containing numeric game data are split into team and opponent specific statistics. The dataframe is then grouped by the 'team' column, and the 'target' feature is shifted to align with future games, representing our prediction target. Finally, any rows containing missing values in key columns are removed.
    </p>
    <h2>Ridge Regression - ridgeregression.py</h2>
    <p>
    The ridgeregression.py script adopts the Ridge Regression model, a regularized linear regression variant that combats overfitting and handles multicollinearity. 
        The model uses a time-series cross-validation method and Sequential Feature Selector for optimal performance assessment and feature selection. 
        A rolling-window strategy is used to account for potential temporal dependencies between games, training, and evaluating the model incrementally as new game data becomes available. 
        The predictive accuracy of this approach is observed to be 0.605.
    </p>
    <h2>Basic XGBoost Model - xg_basic.py</h2>
    <p>
        The xg_basic.py script introduces a Gradient Boosting model using XGBoost (eXtreme Gradient Boosting), a powerful machine learning algorithm based on the gradient boosting framework. It is especially known for its speed and performance.
    </p>
    <p>
        Similar to the Ridge Regression model, data are initially preprocessed and scaled, while the model is trained on data before 2019 and tested on games from 2019 onwards. The script then evaluates the model's predictive accuracy.
    </p>
    <h2>Advanced XGBoost Model - xg_adv.py</h2>
    <p>
        The xg_adv.py script takes a more advanced approach using the XGBoost model. The first step involves scaling and preprocessing the data similarly to the previous scripts. Then, the script uses the GridSearchCV method to tune the hyperparameters of the XGBoost model using a time series cross-validation strategy.
    </p>
    <p>
        Once the optimal hyperparameters are identified, the model is retrained with these settings and used to predict the outcomes of games from 2019 onwards. The model's performance is evaluated based on its accuracy. 
        Walk-forward validation was utilized to simulate a real-world scenario where the model is updated as new data becomes available. For each game in the test set, the model was retrained with all the data available up to that game and then made a prediction for the game's outcome. 
    <p>
        All three scripts aim to predict a target variable based on various features, but they use different strategies and models. The ridgeregression.py script uses a Ridge Classifier and a backtesting strategy to evaluate the model's performance. 
        The xg_basic script uses an XGBoost classifier and splits the data into training and testing sets based on the year. The xg_adv script uses the same type of model, but it also includes hyperparameter tuning and a walk-forward validation strategy.

The accuracy of the models varies, with the Ridge Classifier obtaining an accuracy of 0.605, the basic XGBoost model obtaining an accuracy of 0.572, and the advanced XGBoost model obtaining an accuracy of 
        0.680 (before walk-forward validation, after which the accuracy drops to 0.487). This indicates that the advanced XGBoost model (before walk-forward validation) are the most accurate.
    </p>
</body>
</html>
