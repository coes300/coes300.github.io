<!DOCTYPE html>
<html>
<head>
    <title>Testing Machine Learning Models for Australian Football League Game Predictions</title>
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>
    <h1>Testing Machine Learning Models for Australian Football League Game Predictions</h1>
    <h2>Project Overview</h2>
    <p>Originally made for use in a weekly tipping league, the objective was to leverage machine learning techniques to predict future AFL games.</p>

    <h2>Dataset</h2>
    <p>The primary dataset was downloaded from <a href="https://www.kaggle.com/datasets/gabrieldennis/afl-player-and-game-data-and-statistics18972022">kaggle</a>,
    and consists of AFL game-by-game results from 1965 to 2022.</p>
    
    <h2>Data Preprocessing - data.py</h2>
    <h3>Data Cleaning</h3>
    <p>The raw dataset is loaded from a CSV file 'afl_game_by_game_results_1965_2022.csv'. Several columns, mostly focused on specific game events like 'tackles', 'rebound_50s', 'inside_50s', 'clearances', etc., are removed to reduce complexity and dimensionality. The dataset is then sorted by 'year' and 'round' to maintain chronological order.</p>

    <h3>Feature Engineering</h3>
    <p>Specific columns like 'goals' and 'behinds' are split into separate features for each team. Furthermore, additional fields such as 'team_score' and 'opponent_score' are derived from these. A binary target feature 'won' is created, signifying whether the team won the match or not.</p>

    <h3>Data Finalization</h3>
    <p>Columns containing numeric game data are split into team and opponent specific statistics. The dataframe is then grouped by the 'team' column, and the 'target' feature is shifted to align with future games, representing our prediction target. Finally, any rows containing missing values in key columns are removed.</p>

    <h2>Ridge Regression - rr.py</h2>
    <h3>About Ridge Regression</h3>
    We applied the Ridge Regression model, a variant of the Linear Regression model, which is enhanced with regularization 
    to combat the problem of overfitting. Ridge Regression, also known as L2 regularization, includes an additional term in the loss 
    function -- the square of the magnitude of the coefficients, multiplied by the regularization parameter, \( \alpha \).</p>
    <p>The objective function for Ridge Regression is as follows:</p>
    <p>\[ \min_{w} || X w - y||_2^2 + \alpha ||w||_2^2 \]</p>
    <p>where:</p>
    <ul>
        <li>\(X\) is the matrix of input features,</li>
        <li>\(w\) is the vector of model weights,</li>
        <li>\(y\) is the vector of target values, and</li>
        <li>\(\alpha\) is the regularization parameter.</li>
    </ul>
    <p>This objective function includes two terms: the residual sum of squares (RSS) and the regularization term. The RSS is a measure of the model's fit to the data. 
        The regularization term is a measure of model complexity. The \( \alpha \) parameter controls the trade-off between these two objectives. A higher \( \alpha \) 
        increases the impact of the regularization term, leading to a simpler model with smaller coefficients. A smaller \( \alpha \) places more emphasis on minimizing the RSS, which can lead to a more complex model.</p>
    <p>In this research, we set \( \alpha \) to 1, representing a balanced compromise between model fit and complexity. The Ridge Regression model was fit using the Scikit-learn 
        library, which uses a closed-form solution for this optimization problem, making it computationally efficient.</p>
    <p>The Ridge Regression model was chosen for this research because of its robustness to multicollinearity. With the use of the regularization term, Ridge Regression 
        can manage situations where input features are highly correlated, which is common in the high-dimensional datasets often encountered in sports analytics. 
        This feature of Ridge Regression makes it particularly suitable for the task of predicting Australian Football League game outcomes.</p>
    <img src="ridge_regression.png" alt="Ridge Regression">

    <h3>RR Results</h3>
    <p>Assessing the results achieved with RR, an accuracy of 0.728 signifies that 72.8% of the model's predictions were correct. Precision, at 0.724, reflects that 72.4% of the instances identified as positive were indeed positive. 
    A recall score of 0.732 denotes that 73.2% of actual positive instances were correctly classified. The F1 score, a harmonic mean of precision and recall, stands at 0.728, suggesting a balanced model in terms of both precision and recall.</p>
    <img src="roc_rr.png" alt="Ridge Regression Results"> <img src="cov_rr.png" alt="Ridge Regression Results">

    <h2>Discussion</h2>
    <p>Upon comparison of the results, Ridge Regression demonstrated superior performance in this instance, with higher scores across accuracy, precision, recall, and the F1 score.
    It showcased a robust model, successfully striking a balance between predicting true positives and reducing false positives.</p>
    <img src="comparison_chart.png" alt="Comparison Chart">
</body>
</html>
